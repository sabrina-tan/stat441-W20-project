{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Two methods:\n* Append predictions to modeling dataset, train and predict\n* Use predictions as modeling dataset, train and predict"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport random\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split, cross_validate\nfrom xgboost.sklearn import XGBClassifier\n\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport os"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Append predictions to full data:"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": "# Import predictions\nlgbm_preds_train = pd.read_csv(\"../04-modeling/stacking_predictions/train_fe_data_Final_LGBM_no_rounding.csv\")\nlgbm_preds_test = pd.read_csv(\"../04-modeling/stacking_predictions/test_fe_data_Final_LGBM_no_rounding.csv\")\nxgb_preds_train = pd.read_csv(\"../04-modeling/stacking_predictions/train_fe_data_Final_xgboost_with_CV_no_rounding.csv\")\nxgb_preds_test = pd.read_csv(\"../04-modeling/stacking_predictions/test_fe_data_Final_xgboost_with_CV_no_rounding.csv\")\nrf_preds_train = pd.read_csv(\"../04-modeling/stacking_predictions/train_fe_data_Final_RF_no_rounding.csv\")\nrf_preds_test = pd.read_csv(\"../04-modeling/stacking_predictions/test_fe_data_Final_RF_no_rounding.csv\")\n\n# Import train and test\ntrain = pd.read_csv(\"../03-data-creation/train_FE_final.csv\")\ntest = pd.read_csv(\"../03-data-creation/test_FE_final.csv\")\n\n# Drop unwanted column\ntrain = train.drop(\"Unnamed: 0\", axis = 1)\ntest = test.drop(\"Unnamed: 0\", axis = 1)\n\n# Add predictions rows\ntrain['Predicted_LGBM'] = lgbm_preds_train['Predicted_LGBM']\ntest['Predicted_LGBM'] = lgbm_preds_test['Predicted_LGBM']\ntrain['Predicted_XGB'] = xgb_preds_train['Predicted_XGB']\ntest['Predicted_XGB'] = xgb_preds_test['Predicted_XGB']\ntrain['Predicted_RF'] = rf_preds_train['Predicted_RF']\ntest['Predicted_RF'] = rf_preds_test['Predicted_RF']\n\n# We don't want the ID to be used in the model so preserve\ntrain_id = train['id']\ntest_id = test['id']\n\ntrain = train.drop('id', axis = 1)\ntest = test.drop('id', axis = 1)\n\n# Split X and y\nX = train.loc[:, train.columns != \"satisfied\"]\ny = train.satisfied"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.8070249628950831\n[[3226 1065]\n [ 652 4081]]\n"
    }
   ],
   "source": "# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 3096)\n\n# Single XGB, no CV, default params\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train, y_train)\n\ny_pred = xgb_model.predict(X_test)\n\nroc=metrics.roc_auc_score(y_test, y_pred)\n\nprint(roc)\nprint(metrics.confusion_matrix(y_test, y_pred))"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": "# Guide to HyperOpt: https://www.kaggle.com/yassinealouini/hyperopt-the-xgboost-model\n# Some constants\nSEED = 314159265\nVALID_SIZE = 0.2\nTARGET = 'satisfied'\n\n# Utility functions\n\ndef intersect(l_1, l_2):\n    return list(set(l_1) & set(l_2))\n\ndef get_features(train, test):\n    intersecting_features = intersect(train.columns, test.columns)\n    return sorted(intersecting_features)\n\n# Scoring and optimization functions\n\n\ndef score(params):\n    print(\"Training with params: \")\n    print(params)\n    num_round = int(params['n_estimators'])\n    del params['n_estimators']\n    dtrain = xgb.DMatrix(train_features, label=y_train)\n    dvalid = xgb.DMatrix(valid_features, label=y_valid)\n    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n    gbm_model = xgb.train(params, dtrain, num_round,\n                          evals=watchlist,\n                          verbose_eval=True)\n    predictions = gbm_model.predict(dvalid,\n                                    ntree_limit=gbm_model.best_iteration + 1)\n    score = roc_auc_score(y_valid, predictions)\n    # TODO: Add the importance for the selected features\n    print(\"\\tScore {0}\\n\\n\".format(score))\n    # The score function should return the loss (1-score)\n    # since the optimize function looks for the minimum\n    loss = 1 - score\n    return {'loss': loss, 'status': STATUS_OK}\n\n\ndef optimize(\n             #trials, \n             random_state=SEED):\n    \"\"\"\n    This is the optimization function that given a space (space here) of \n    hyperparameters and a scoring function (score here), finds the best hyperparameters.\n    \"\"\"\n    # To learn more about XGBoost parameters, head to this page: \n    # https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n    space = {\n        'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n        # A problem with max_depth casted to float instead of int with\n        # the hp.quniform method.\n        'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic',\n        # Increase this number if you have more cores. Otherwise, remove it and it will default \n        # to the maxium number. \n        'nthread': 4,\n        'booster': 'gbtree',\n        'tree_method': 'exact',\n        'silent': 1,\n        'seed': random_state\n    }\n    # Use the fmin function from Hyperopt to find the best hyperparameters\n    best = fmin(score, space, algo=tpe.suggest, \n                # trials=trials, \n                max_evals=50)\n    return best"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The training set is of length:  15400\nThe validation set is of length:  3850\n"
    }
   ],
   "source": "FEATURES = get_features(train, test)\n\n# Extract the train and valid (used for validation) dataframes from the train_df\n\ntrain, valid = train_test_split(train, test_size=VALID_SIZE,\n                                random_state=SEED)\ntrain_features = train[FEATURES]\nvalid_features = valid[FEATURES]\ny_train = train[TARGET]\ny_valid = valid[TARGET]\n\nprint('The training set is of length: ', len(train.index))\nprint('The validation set is of length: ', len(valid.index))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": "best_hyperparams = optimize(\n                            #trials\n                            )\nprint(\"The best hyperparameters are: \", \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'colsample_bytree': 0.8, 'eta': 0.05, 'gamma': 0.6000000000000001, 'max_depth': 1, 'min_child_weight': 3.0, 'n_estimators': 184.0, 'subsample': 0.65}\n"
    }
   ],
   "source": "print(best_hyperparams)"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.8, eta=0.05, gamma=0.6,\n              gpu_id=-1, importance_type='gain', interaction_constraints=None,\n              learning_rate=0.0500000007, max_delta_step=0, max_depth=1,\n              min_child_weight=3, missing=nan, monotone_constraints=None,\n              n_estimators=184, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=0.65,\n              tree_method=None, validate_parameters=False, verbosity=None)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Fit model & predict\n\ntuned_model = xgb.XGBClassifier(objective=\"binary:logistic\",\n                               colsample_bytree = 0.8,\n                               eta = 0.05,\n                               gamma = 0.6,\n                               max_depth = 1,\n                               min_child_weight = 3,\n                               n_estimators = 184,\n                               subsample = 0.65)\ntuned_model.fit(X, y)"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": "test_new = test.copy()\ntest_preds = pd.DataFrame(tuned_model.predict(test))\ntest_new['id'] = test_id\ntest_new['Predicted'] = test_preds\n\ntest_new[['id', 'Predicted']].to_csv('/Users/sabrinatan/Desktop/STAT 441/stat441-W20-project/04-modeling/stacking_predictions/stacked_XGB_1_FullData.csv', index = False, float_format = \"%.8f\")\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Using only the predictions:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# New DFs\nmodel_reduced = ['Predicted_LGBM', 'Predicted_XGB', 'Predicted_RF', 'satisfied']\ntrain_reduced = train[model_reduced]\ntrain_reduced = test[model_reduced]\n\n# Split X and y\nX_reduced = train_reduced.loc[:, train_reduced.columns != \"satisfied\"]\ny_reduced = train_reduced.satisfied"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split data\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reduced, y_reduced, test_size=0.30, random_state = 3096)\n\n# Single XGB, no CV, default params\nxgb_model_r = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_mode_rl.fit(X_train_r, y_train_r)\n\ny_pred_r = xgb_model.predict(X_test_r)\n\nroc_r = metrics.roc_auc_score(y_test_r, y_pred_r)\n\nprint(roc_r)\nprint(metrics.confusion_matrix(y_test_r, y_pred_r))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FEATURES = get_features(train_reduced, test_reduced)\n\n# Extract the train and valid (used for validation) dataframes from the train_df\n\ntrain_r, valid_r = train_test_split(train_reduced, test_size=VALID_SIZE,\n                                random_state=SEED)\ntrain_features_r = train_r[FEATURES]\nvalid_features_r = valid_r[FEATURES]\ny_train_r = train_r[TARGET]\ny_valid_r = valid_r[TARGET]\n\nprint('The training set is of length: ', len(train_r.index))\nprint('The validation set is of length: ', len(valid_r.index))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "best_hyperparams_r = optimize(\n                            #trials\n                            )\nprint(\"The best hyperparameters are: \", \"\\n\")\nprint(best_hyperparams_r)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
