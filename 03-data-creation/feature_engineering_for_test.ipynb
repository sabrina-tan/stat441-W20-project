{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport pycountry_convert as pc\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\n\n# Set options\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import data\ntest = pd.read_csv(\"../01-data/test.csv\", low_memory = False)\n\n# Custom data\ncodebook = pd.read_csv(\"../01-data/codebook_compact.csv\", low_memory = False) # OG codebook+dtypes from codebook_long"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": "# Dealing with country vars on a case-by-case basis\ncat_cols = ['v17', \"v20\", \"v25\", \"v78\", \"v154\", \"v155\", \"v161\", \"cntry\"]\ncat_df = test[cat_cols]\n\n## V17: Country of Birth\n\n# Drop 99, 2, 4, 3, 77, 6\nto_drop = ['99', '2', '4', '3', '77', '6', '88']\ntest['v17'] = test['v17'].replace(to_drop, \".\")\n\ndef convert_country(country):\n    try:\n        return(pc.country_alpha2_to_continent_code(country))\n    except:\n        return(country)\n\ntest['v17'] = test['v17'].apply(lambda x: convert_country(x))\n\n# Only one observation is from TL, and none exist in the test set. Drop ID = 2441\ntest = test[test.v17 != 'TL']\n\ntest['v17'].value_counts()\n\n## V20: Region\n\n# Country code + more specific region code. Convert to just country\n# After stripping specific region code, only 6% of obs have a different entry here than for 'cntry'. Drop this column.\ntest = test.drop('v20', axis = 1)\n\n## V25: Citizenship\n\n# Drop 65, 99, 6, convert rest to continents\nto_drop = ['65', '99', '6', '88']\ntest['v25'] = test['v25'].replace(to_drop, '.')\ntest['v25'] = test['v25'].apply(lambda x: convert_country(x))\n\n## V78: Country of birth, father AND V161: Country of birth, mother\n\n# Drop errors, convert rest to continents\nto_drop = ['99', '88', '2', '4', '3', '77', '6']\ntest['v78'] = test['v78'].replace(to_drop, '.')\ntest['v78'] = test['v78'].apply(lambda x: convert_country(x))\n\ntest['v161'] = test['v161'].replace(to_drop, '.')\ntest['v161'] = test['v161'].apply(lambda x: convert_country(x))\n\n## V154: Language most often spoken at home: first mentioned\n\n# Drop errors\nto_drop = ['999','888','777']\ntest['v154'] = test['v154'].replace(to_drop, '.')\n\n# Leaving ENG and GER as is \nmissing = [\".\"]\nGER = [\"GER\"]\nENG = [\"ENG\"]\neur = [\"FRE\",\"DUT\",\"SPA\",\"HUN\",\"POR\",\"GSW\",\"CAT\",\"ITA\",\"GLG\",\"ALB\",\"FRS\",\"HRV\",\"BAQ\",\"ROA\",\"GLE\",\n       \"GEM\",\"CPP\",\"FRM\",\"GRE\",\"WEL\",\"ROH\",\"BEL\",\"FAO\",\"SRN\",\"NAP\",\"AST\",\"FRO\"]\nbal = [\"CZE\", \"LIT\"]\nafr = [\"AMH\",\"SOM\",\"BER\",\"MLG\",\"TIR\",\"MAN\",\"LIN\",\"WOL\",\"BAM\",\"AKA\",\"SWA\",\"SUX\",\"SUS\",\"SNK\",\"TIG\"]\nnor = [\"FIN\",\"SWE\",\"DAN\",\"NOR\",\"EST\",\"ICE\"]\nasia = [\"CHI\",\"VIE\",\"THA\",\"MAY\",\"TAI\",\"TIB\",\"IND\",\"JPN\",\"LAO\"]\nsoutha = [\"URD\",\"HIN\",\"TAM\",\"GUJ\",\"BEN\",\"MAL\",\"PAN\",\"TEL\",\"CPF\"]\nmideast = [\"HEB\",\"ARA\",\"TUR\",\"KUR\",\"PER\",\"ARM\",\"IRA\",\"YID\",\"SYR\",\"AZE\",\"UZB\",\"TGK\",\"KAB\",\"EGY\",\"ABK\",\"GEO\"]\nslav = [\"POL\",\"SLV\",\"RUS\",\"SRP\",\"BOS\",\"RUM\",\"HRV\",\"SLO\",\"ROM\",\"BUL\",\"UKR\",\"SLA\",\"LAV\",\"MDR\"]\noth = [\"APA\",\"PAP\",\"MIS\",\"TGL\",\"NEP\",\"FIL\",\"MON\"]\n\nlanguages = {language: 'EUR' for language in eur}\nlanguages.update({language: '.' for language in missing})\nlanguages.update({language: 'GER' for language in GER})\nlanguages.update({language: 'ENG' for language in ENG})\nlanguages.update({language: 'BAL' for language in bal})\nlanguages.update({language: 'AFR' for language in afr})\nlanguages.update({language: 'NOR' for language in nor})\nlanguages.update({language: 'ASIA' for language in asia})\nlanguages.update({language: 'SOUTHA' for language in southa})\nlanguages.update({language: 'MIDEAST' for language in mideast})\nlanguages.update({language: 'SLAV' for language in slav})\nlanguages.update({language: 'OTHER' for language in oth})\n\ntest['v154'] = test['v154'].map(languages)\n\ntest['v154'].value_counts(normalize = True)\n\n## V155: Language most often spoken at home: second mentioned\n\n# Assuming 0 means no other language, and 999, 888, 777 are missing\nto_drop = ['999','888','777']\ntest['v155'] = test['v155'].replace(to_drop, '.')\n\n## Check if any weren't present in the previous question\nlang_v154 = [\"GER\",\"ENG\",\"FRE\",\"CZE\",\"LIT\",\"FIN\",\"POL\",\"DUT\",\"SWE\",\"HEB\",\"SPA\",\n             \"HUN\",\"DAN\",\"NOR\",\"POR\",\"EST\",\"SLV\",\"RUS\",\"GSW\",\"ARA\",\"CAT\",\"TUR\",\n             \"ITA\",\"GLG\",\"SRP\",\"BOS\",\"ALB\",\"FRS\",\"RUM\",\"KUR\",\"HRV\",\"BAQ\",\"URD\",\n             \"PER\",\"AMH\",\"APA\",\"ROA\",\"CHI\",\"HIN\",\"ARM\",\"GLE\",\"SOM\",\"SLO\",\"IRA\",\n             \"TAM\",\"GEM\",\"GUJ\",\"ROM\",\"CPP\",\"FRM\",\"GRE\",\"WEL\",\"VIE\",\"BUL\",\"UKR\",\n             \"YID\",\"BER\",\"PAP\",\"MIS\",\"BEN\",\"SLA\",\"LAV\",\"MAL\",\"THA\",\"TGL\",\"NEP\",\n             \"MAY\",\"SYR\",\"AZE\",\"UZB\",\"PAN\",\"TGK\",\"MLG\",\"ROH\",\"TIR\",\"MAN\",\"TAI\",\n             \"LIN\",\"KAB\",\"EGY\",\"BEL\",\"WOL\",\"BAM\",\"TIB\",\"AKA\",\"SWA\",\"IND\",\"SUX\",\n             \"TEL\",\"FAO\",\"JPN\",\"SRN\",\"SUS\",\"ICE\",\"SNK\",\"FIL\",\"ABK\",\"GEO\",\"TIG\",\n             \"NAP\",\"AST\",\"FRO\",\"LAO\",\"CPF\",\"MON\",\"MDR\"]\n\nlang_v155 = test['v155'].unique().tolist()\n\nnew_languages = [language for language in lang_v155 if language not in lang_v154]\n\n# New language dict: leaving ENG and GER as is \nnone = ['0']\nmissing = [\".\"]\nGER = [\"GER\"]\nENG = [\"ENG\"]\neur = [\"FRE\",\"DUT\",\"SPA\",\"HUN\",\"POR\",\"GSW\",\"CAT\",\"ITA\",\"GLG\",\"ALB\",\"FRS\",\"HRV\",\"BAQ\",\"ROA\",\"GLE\",\n       \"GEM\",\"CPP\",\"FRM\",\"GRE\",\"WEL\",\"ROH\",\"BEL\",\"FAO\",\"SRN\",\"NAP\",\"AST\",\"FRO\",\"ENM\",\"OCI\",\"BRE\",\n       \"NDS\",\"WLN\",\"VOL\",\"MWL\"]\nbal = [\"CZE\", \"LIT\", \"MAC\", \"CSB\"]\nafr = [\"AMH\",\"SOM\",\"BER\",\"MLG\",\"TIR\",\"MAN\",\"LIN\",\"WOL\",\"BAM\",\"AKA\",\"SWA\",\"SUX\",\"SUS\",\"SNK\",\"TIG\",\n       \"EWE\",\"RUN\",\"IBO\",\"YOR\",\"FON\",\"KON\",\"DYU\",\"KIN\",\"NIC\",\"NDE\",\"HER\",\"KIK\"]\nnor = [\"FIN\",\"SWE\",\"DAN\",\"NOR\",\"EST\",\"ICE\", 'SMN']\nasia = [\"CHI\",\"VIE\",\"THA\",\"MAY\",\"TAI\",\"TIB\",\"IND\",\"JPN\",\"LAO\"]\nsoutha = [\"URD\",\"HIN\",\"TAM\",\"GUJ\",\"BEN\",\"MAL\",\"PAN\",\"TEL\",\"CPF\", \"MAR\"]\nmideast = [\"HEB\",\"ARA\",\"TUR\",\"KUR\",\"PER\",\"ARM\",\"IRA\",\"YID\",\"SYR\",\"AZE\",\"UZB\",\"TGK\",\"KAB\",\"EGY\",\"ABK\",\"GEO\"]\nslav = [\"POL\",\"SLV\",\"RUS\",\"SRP\",\"BOS\",\"RUM\",\"HRV\",\"SLO\",\"ROM\",\"BUL\",\"UKR\",\"SLA\",\"LAV\",\"MDR\"]\noth = [\"APA\",\"PAP\",\"MIS\",\"TGL\",\"NEP\",\"FIL\",\"MON\",\"ZXX\",\"TPI\",\"GRN\",\"SGN\",\"HUP\",\"NAH\",\"HAT\",\"CAD\",\"PHI\",\"MAO\"]\n\n \nlanguages = {language: 'EUR' for language in eur}\nlanguages.update({language: '0' for language in none})\nlanguages.update({language: '.' for language in missing})\nlanguages.update({language: 'GER' for language in GER})\nlanguages.update({language: 'ENG' for language in ENG})\nlanguages.update({language: 'BAL' for language in bal})\nlanguages.update({language: 'AFR' for language in afr})\nlanguages.update({language: 'NOR' for language in nor})\nlanguages.update({language: 'ASIA' for language in asia})\nlanguages.update({language: 'SOUTHA' for language in southa})\nlanguages.update({language: 'MIDEAST' for language in mideast})\nlanguages.update({language: 'SLAV' for language in slav})\nlanguages.update({language: 'OTHER' for language in oth})\n\n# Map langauges to their groups\ntest['v155'] = test['v155'].map(languages)\n\ntest['v155'].value_counts(normalize = True)\n\n## V3: Age of respondent, calculated\n\nmissing_age = round(len(test[test['v3'] == '.a'])/len(test) * 100, 2)\nprint(str(missing_age) + \"% of the ages are missing.\")\n\n# Using EDA notebook, the highest positive and negatively linearly correlated vars with Age (v3):\n# Positive: v217, v163, v218, v103, v99\n# Negative: v54, v196, v208, v100, v159\t\ncols = ['v3','v217','v218', 'v54']\n\nregress_age = test[cols]\ntrain_lr = regress_age[regress_age['v3'] != '.a']\ntest_lr = regress_age[regress_age['v3'] == '.a']\n\nfor col in cols:\n    train_lr[col] = pd.to_numeric(test[col], errors = 'coerce')\n\n# Drop all na\ntrain_lr = train_lr.dropna()\n\nX_train = train_lr.loc[:, train_lr.columns != 'v3']\ny_train = train_lr['v3']\n\n# Linear regression\nregressor = lgb.LGBMRegressor()  \nregressor.fit(X_train, y_train) \n\ndel test_lr['v3']\ntest_lr = test_lr.replace(['.a', '.b', '.c', '.d'], np.nan).dropna()\n\ntest_lr[\"predicted_age\"] = regressor.predict(test_lr)\n\n# Gives R2 score of 0.63 but better than imputation by mean\n# metrics.r2_score(label, train_no_na[\"predicted_age\"])\n\n# Filling in missing values with predicted age\nindex_predicted = test_lr.drop(['v217','v218', 'v54'], axis = 1).index\nindex_missing = test[test['v3'] == '.a'].index\n\ntest = test.join(test_lr['predicted_age'])\ntest['v3_imputed'] = np.where(test['v3'] == '.a', test['predicted_age'], test['v3']).astype(\"float64\")\n\n# Delete other versions of age\ndel test['v3']\ndel test['predicted_age']\n\n# Plot age distribution\nsns.set(color_codes=True)\nsns.distplot(test['v3_imputed'].astype(\"float64\"));\n\n# (37.0, 44.0]\n# (23.0, 31.0]\n# (44.0, 50.0]\n# (61.0, 67.0]\n# (55.0, 61.0]\n# (13.999, 23.0]\n# (74.423, 114.0]\n# (67.0, 74.423]\n# (31.0, 37.0]\n# (50.0, 55.0]\n\nbin_labels = [1,2,3,4,5,6,7,8,9,10]\ntest['v3_binned'] = pd.qcut(test['v3_imputed'], q=[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1], labels=bin_labels)\n\ndel test['v3_imputed']\n\n# Drop v102 as it is almost the same as v103 but with less information\ndel test['v102']\n\n# Drop v57, v59, v61, v63 as v56, v58, v60, v62 gives the same information but mapped to categories\ndel test['v57']\ndel test['v59']\ndel test['v61']\ndel test['v63']\n\n# V64: Years of full time education completed\n\n# Combine all missing\ntest['v64'] = pd.to_numeric(test['v64'], errors='coerce')\n\nmissing_age = round(len(test[test['v64'].isnull()])/len(test) * 100, 2)\nprint(str(missing_age) + \"% of the years are missing.\")\n\n# Plot distribution\nsns.distplot(test['v64'].fillna(-1));\n\n# Fill with mean bc I'm tired\ntest['v64'] = pd.to_numeric(test['v64'], errors = 'coerce')\nmean_years = np.nanmean(test['v64'])\ntest['v64'] = test['v64'].fillna(mean_years)\n\n## Now bin education\ntest['v64'].describe()\n\nbin_labels = [1,2,3,4,5]\ntest['v64_binned'] = pd.qcut(test['v64'], q=[0,0.2,0.4,0.6,0.8,1], labels=bin_labels)\n\ntest['v64_binned'].value_counts()\ndel test['v64']\n\n## V100: Number of people living regularly as member of household\n# Group everything more than 6 into \"6+\", i.e. 6\n\n# Combine all missing\ntest['v100'] = pd.to_numeric(test['v100'], errors='coerce')\n\nmissing_age = round(len(test[test['v100'].isnull()])/len(test) * 100, 2)\nprint(str(missing_age) + \"% of the numbers are missing.\")\n\n# Fill with mean\nmean_years = np.nanmean(test['v100'])\ntest['v100'] = test['v100'].fillna(mean_years).astype(\"int64\")\n\n# Grouping large values\ntest['v100_grouped'] = np.where(test['v100'] > 6, 6, test['v100']).astype(\"int64\")\ntest['v100_grouped'].value_counts()\n\n# v128: End of interview, month\n# v129: Start of interview, month\n# End and start of interview are only not equal for 0.18% of observations\n# We keep v129 since there are no missing\ndel test['v128']\n\n# Define: 3-5 = Spring, 6-8 = Summer, 9-11 = Autumn, 12-2 = Winter - in Germany\nwinter = [12,1,2]\nspring = [3,4,5]\nsummer = [6,7,8]\nautumn = [9,10,11]\n\n# Dictionary for seasons\nseasons = {season: 'WI' for season in winter}\nseasons.update({season: 'SP' for season in spring})\nseasons.update({season: 'SU' for season in summer})\nseasons.update({season: 'AU' for season in autumn})\n\n# Map langauges to their groups\ntest['start_interview_season'] = test['v129'].map(seasons)\ntest['start_interview_season'].value_counts(normalize = True)\n\n# Delete original variable\ndel test['v129']\n\n# v126: end of interview, hour\n# v130: start of interview, hour\n\n# Impute with mean\ntest['v126'] = pd.to_numeric(test['v126'], errors='coerce')\ntest['v126'] = test['v126'].fillna(np.nanmean(test['v126'])).astype('int64')\ntest['v130'] = pd.to_numeric(test['v130'], errors='coerce')\ntest['v130'] = test['v130'].fillna(np.nanmean(test['v130'])).astype('int64')\n\nbin_labels = [1,2,3,4,5]\ntest['v126_binned'] = pd.qcut(test['v126'], q=[0,0.2,0.4,0.6,0.8,1], labels=bin_labels)\ntest['v130_binned'] = pd.qcut(test['v130'], q=[0,0.2,0.4,0.6,0.8,1], labels=bin_labels)\n\n# Delete original variables\ndel test['v126']\ndel test['v130']\n\n# v132: Interview length in minutes - bin in quantiles\nbin_labels = [1,2,3,4,5,6,7,8,9,10]\ntest['v132_binned'] = pd.qcut(test['v132'], q=[0,.1,.2,.3,.4,.5,.6,.7,.8,.9,1], labels=bin_labels)\n\n# Delete original variable\ndel test['v132']\n\n# v250: Total contracted hours per week in main job OT excluded\ntest['v250'] = pd.to_numeric(test['v250'], errors = 'coerce')\ntest['v250_binned'] = pd.cut(test['v250'], 4, labels=[1,2,3,4])\n\ndel test['v250']\n\n# v251: Total hours normally worked per week main job OT included\ntest['v251'] = pd.to_numeric(test['v251'], errors = 'coerce')\ntest['v251_binned'] = pd.cut(test['v251'], 4, labels=[1,2,3,4])\n\ndel test['v251']\n\n# v251: Total hours normally worked per week main job OT included - partner\ntest['v252'] = pd.to_numeric(test['v252'], errors = 'coerce')\ntest['v252_binned'] = pd.cut(test['v252'], 4, labels=[1,2,3,4])\n\ndel test['v252']\n\n# v168: Number of people responsible for in job\ntest['v168'] = pd.to_numeric(test['v168'], errors = 'coerce')\ntest['v168'] = test['v168'].fillna(0).astype('int64')\n\n# Define:\nnone = [0]\nfew = [1,2,3,4,5,6,7,8,9,10]\nmoderate = [11,12,13,14,15,16,17,18,19,20]\nmany = range(21, max(test['v168']))\n\n# Dictionary for seasons\npeople = {people: 0 for people in none}\npeople.update({people: 1 for people in few})\npeople.update({people: 2 for people in moderate})\npeople.update({people: 3 for people in many})\n\n# Map langauges to their groups\ntest['manage'] = test['v168'].map(people)\ntest['manage'].value_counts(normalize = True)\n\n# Delete original variable\ndel test['v168']\n\ntest['v153'] = pd.to_numeric(test['v153'], errors = 'coerce')\ntest['v153'] = test['v153'].fillna(0).astype('int64')\n\n# Define:\nna = [0]\nbefore_80 = range(1, 1980)\nafter_80 = range(1981, max(test['v153']))\n\n# Dictionary for seasons\nimmigrate = {immigrate: 0 for immigrate in na}\nimmigrate.update({immigrate: 1 for immigrate in before_80})\nimmigrate.update({immigrate: 2 for immigrate in after_80})\n\n# Map langauges to their groups\ntest['immigrate'] = test['v153'].map(immigrate)\ntest['immigrate'].value_counts(normalize = True)\n\n# Delete original variable\ndel test['v153']\n\ntest['single'] = np.where(test['v164'] == '.a', 1, 0)\n\ndots = ['.a', '.b', '.c', '.d']\ntest['employees'] = np.where(np.isin(test['v69'], dots) , 0, 1)\n\n# Convert binned vars to int to fillna\nbinned = [\"v3_binned\",\"v64_binned\", \"v126_binned\", \"v130_binned\", \"v132_binned\", \n          \"v250_binned\", \"v251_binned\", \"v252_binned\"]\n\nfor col in binned:\n    test[col] = test[col].astype('int64')\n\ntest = test.fillna(\".\")\n\n# Treating missing \n\ntest_nodots = test.replace([\".\", \".a\", \".b\", \".c\", \".d\"], [np.nan, np.nan, np.nan, np.nan, np.nan])\npercent_missing_nodots = test_nodots.isnull().sum() * 100 / len(test_nodots)\nmissing_value_df_nodots = pd.DataFrame({'column_name': test_nodots.columns,\n                                 'percent_missing': percent_missing_nodots})\n\n# Columns where % missing > some percent\npercent = 50\ncols_missing = missing_value_df_nodots[missing_value_df_nodots.percent_missing > percent]\nn_cols_missing = len(missing_value_df_nodots[missing_value_df_nodots.percent_missing > percent])\nprint(\"There are \" + str(n_cols_missing) + \" features with over \" + str(percent) + \"% missing.\")\n\n# Attach short desc for more context\ncodebook_labels = ['Variable', \"Label\"]\nmissing = cols_missing.merge(codebook[codebook_labels], left_on = 'column_name', right_on = \"Variable\", how = \"left\")\nmissing.sort_values(by = \"percent_missing\", ascending = False)\n\n# Drop\ndrop_missing = cols_missing.column_name.tolist()\ntest_dropped = test_nodots.drop(drop_missing, axis = 1)\n\n# Impute the rest\ndots = [\".\", \".a\", \".b\", \".c\", \".d\", np.nan]\ntest_imputed = test_dropped.replace(dots, [-1, -1, -1, -1, -1, -1])\n\n# quick fix\ntest_imputed['v252_binned'] = np.where(test_imputed['v252_binned'] < 1, 0, test_imputed['v252_binned'])\ntest_imputed['v251_binned'] = np.where(test_imputed['v251_binned'] < 1, 0, test_imputed['v251_binned'])\ntest_imputed['v250_binned'] = np.where(test_imputed['v250_binned'] < 1, 0, test_imputed['v250_binned'])\n\none_hot = ['v17','v25','v78','v161', 'v154', 'v155', 'start_interview_season']\ntest_encoded = pd.get_dummies(test_imputed, prefix=one_hot, columns=one_hot)\n\n# Label encode country\nlabel_encoder = preprocessing.LabelEncoder()\ntest_encoded['cntry'] = label_encoder.fit_transform(test_encoded['cntry'])\n    \n# Convert all other variables to int64\nnum_cols = test_encoded.loc[:, ~test_encoded.columns.isin(['cntry'])].columns.tolist()\ntest_encoded[num_cols] = test_encoded[num_cols].astype(\"int64\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "test_encoded.to_csv(\"test_FE.csv\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
