{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom pandas.api.types import CategoricalDtype\n\n# Set options\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 4000\n\n# Import data\ntrain = pd.read_csv(\"../01-data/train.csv\", low_memory = False)\ntest = pd.read_csv(\"../01-data/test.csv\", low_memory = False)"
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": "# Join train and test for processing - otherwise they end up with a different # of columns\ntrain_labels = train['satisfied']\ndel train['satisfied']\ntrain['data'] = 1\ntest['data'] = 0"
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": "all_data = train.append(test)"
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": "# 0,1 or 1/2 - just need to treat NA\nbinary = ['v6','v7','v8','v9','v10','v11','v12','v14','v15','v16','v18', 'v21','v22','v23','v24','v26','v27',\n         'v28','v29','v30','v31','v32','v33','v34','v36','v37','v38','v39','v40','v41','v42','v43','v44','v45',\n         'v46','v47','v48','v49','v50','v51','v52','v53','v54','v55','v77','v85','v90','v91','v92','v93','v94',\n         'v106','v107','v108','v123','v152','v157','v162','v165','v166','v171','v172','v173','v175','v176','v187',\n         'v188','v217','v218','v221','v241','v242','v243','v244','v245','v246','v247','v254','v256','v257']\n\n# No order - create \"other\" col and treat NA\nnominal = ['v4','v5','v17','v20','v25','v57','v59','v61','v63','v70','v71','v72','v73','v78','v102','v103','v150',\n          'v151','v154','v155','v158','v159','v160','v161','v163','v164','v167','v169','v170','v174','v190','v191',\n          'v196','v197','v198','v199','v208','v209','v210','v211','v216','v231','v248','v255','cntry']\n\n# Just treat NA\nnumeric = ['v3','v64','v69','v100','v124','v125','v126','v127','v128','v129','v130','v131','v132','v134','v168',\n           'v228','v229','v230','v250','v251','v252','v133']\n\n# Has an order - create \"other\" col and treat NA\nordinal = ['v1','v2','v13','v19', 'v35','v56','v58','v60','v62','v65','v66','v67','v68','v74','v75','v79',\n          'v80','v81','v82','v83','v84','v99','v101','v104','v105','v110','v111','v112','v113','v114','v115',\n          'v116','v117','v118','v119','v122','v135','v136','v137','v138','v139','v140','v141','v142','v143',\n          'v144','v145','v146','v147','v148','v149','v177','v181','v182','v185','v186','v219','v220',\n          'v222','v223','v224','v225','v226','v227','v232','v233','v234','v235','v236','v237','v238','v239',\n          'v240','v249','v253','v258','v263','v264','v265','v266','v76','v98','v109','v120','v121','v153','v156',\n          'v178','v179','v180','v184','v189','v183']\n\n# Drop - 5th+ member of household vars\nto_drop = ['v86','v87','v88','v89','v95','v96','v97','v192','v193','v194','v195','v200','v201','v202','v203',\n          'v204','v205','v206','v207','v212','v213','v214','v215','v259','v260','v261','v262','v267','v268','v269',\n           'v270']\n\n# Drop unwanted vars\nall_data = all_data.drop(to_drop, axis = 1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## process:"
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[]\n"
    },
    {
     "data": {
      "text/plain": "(39325, 3469)"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "#### Binary vars - 0,1 or 1,2\n\n# Replace NaN, \".c\", \".d\" with \".b\" - corresponds with \"No Answer\"\nall_data[binary] = all_data[binary].fillna(\".b\")\nall_data[binary] = all_data[binary].replace(\".c\", \".b\")\nall_data[binary] = all_data[binary].replace(\".d\", \".b\")\n\n# One-hot encode them \nall_data_encoded_1 = pd.get_dummies(all_data, prefix=binary, columns=binary)\n\n#### Nominal (no order) - create \"other\" column and treat NA\n\n# Replace NaN, \".c\", \".d\" with \".b\" - corresponds with \"No Answer\"\nall_data_encoded_1[nominal] = all_data_encoded_1[nominal].fillna(\".b\")\nall_data_encoded_1[nominal] = all_data_encoded_1[nominal].replace(\".\", \".b\")\nall_data_encoded_1[nominal] = all_data_encoded_1[nominal].replace(\".c\", \".b\")\nall_data_encoded_1[nominal] = all_data_encoded_1[nominal].replace(\".d\", \".b\")\n\n# One-hot encode them \nall_data_encoded_2 = pd.get_dummies(all_data_encoded_1, prefix=nominal, columns=nominal)\n\n#### Numeric - Just treat NA\n\n# v69,v168,v250,v251,252: .a corresponds to NA, i.e. 0\nall_data_encoded_2['v69'] = all_data_encoded_2['v69'].replace(\".a\", 0)\nall_data_encoded_2['v168'] = all_data_encoded_2['v168'].replace(\".a\", 0)\nall_data_encoded_2['v250'] = all_data_encoded_2['v250'].replace(\".a\", 0)\nall_data_encoded_2['v251'] = all_data_encoded_2['v251'].replace(\".a\", 0)\nall_data_encoded_2['v252'] = all_data_encoded_2['v252'].replace(\".a\", 0)\n\n# Now get rid of the rest of NA\nall_data_encoded_2[numeric] = all_data_encoded_2[numeric].replace([\".a\",\".b\",\".c\",\".d\",\".\"], [np.nan,np.nan,np.nan,np.nan,np.nan])\n\n# Impute rest of the NA with mean\nfor col in numeric:\n    all_data_encoded_2[col] = pd.to_numeric(all_data_encoded_2[col], errors = \"coerce\")\n    colmean = np.nanmean(all_data_encoded_2[col])\n    all_data_encoded_2[col] = all_data_encoded_2[col].fillna(colmean)\n\n## TAKES REALLY LONG TO RUN\n# # Now smart impute the rest\n\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n\n# features = list(set(all_data_encoded_2.columns.tolist()) - set(binary) - set(nominal) - set(ordinal))\n\n# imp = IterativeImputer(ExtraTreesRegressor(n_estimators=10, random_state=0),verbose=2)\n# imp.fit(all_data_encoded_2[features])\n# imputed_df = imp.transform(all_data_encoded_2[features])\n# imputed_df = pd.DataFrame(imputed_df, columns=all_data_encoded_2[features].columns)\n\n#### Ordinal - create \"other\" column, treat NA, set as category type\n\n# Replace NaN, \".c\", \".d\" with \".b\" - corresponds with \"No Answer\"\nall_data_encoded_2[ordinal] = all_data_encoded_2[ordinal].fillna(\".b\")\nall_data_encoded_2[ordinal] = all_data_encoded_2[ordinal].replace(\".c\", \".b\")\nall_data_encoded_2[ordinal] = all_data_encoded_2[ordinal].replace(\".d\", \".b\")\n\n# Impute all .a with 0\nall_data_encoded_2[ordinal] = all_data_encoded_2[ordinal].replace(\".a\", 0)\n\n# Now get rid of the rest of NA\nall_data_encoded_2[ordinal] = all_data_encoded_2[ordinal].replace([\".b\",\".c\",\".d\",\".\"], [np.nan,np.nan,np.nan,np.nan])\n\n# Impute all rest NA with mean\nfor col in ordinal:\n    all_data_encoded_2[col] = pd.to_numeric(all_data_encoded_2[col], errors = \"coerce\")\n    colmean = np.nanmean(all_data_encoded_2[col])\n    all_data_encoded_2[col] = all_data_encoded_2[col].fillna(round(colmean))\n\n# Convert to category, preserving order\ncat_type = CategoricalDtype(categories=range(0,3000),ordered=True)\nall_data_encoded_2[ordinal] = all_data_encoded_2[ordinal].astype(cat_type)\n\n## Checking if any missing/nan remaining, and shape\nnan_values = all_data_encoded_2.isna()\nnan_columns = nan_values.any()\n\ncolumns_with_nan = all_data_encoded_2.columns[nan_columns].tolist()\nprint(columns_with_nan)\nall_data_encoded_2.shape\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## split:"
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
    }
   ],
   "source": "test = all_data_encoded_2[all_data_encoded_2['data']==0]\ntrain = all_data_encoded_2[all_data_encoded_2['data']==1]\ntrain['satisfied'] = train_labels\n\ndel test['data']\ndel train['data']"
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": "test.to_csv(\"test_min_processing.csv\")\ntrain.to_csv(\"train_min_processing.csv\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
