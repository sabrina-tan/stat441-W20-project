{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data Cleaning Steps\n  \nTwo datasets will be created; one with all variables encoded to numeric values and missing values imputed for models such as regression, KNN, SVM. Another with minimal data preprocessing for the boosting algorithms. the same processing must be done on the test dataset.  \n\n#### Dataset 1\n1) Remove variables/columns that have more than 30% of responses missing\n\n#### Dataset 2\n\n1) Categorize any missing response from (NA, \"\", ., .a, .b, .c, .d) to -1  \n2) Remove variables/columns that have more than 30% of responses missing  \n4) Encode country and language responses  \n5) Clean data types of variables/columns to corresponding str/float/int64  \n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Dataset 1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "# Import packages\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\n\n# Set options\npd.options.display.max_rows = 999"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "# Import data\ntrain = pd.read_csv(\"../01-data/train.csv\", low_memory = False)\ntest = pd.read_csv(\"../01-data/test.csv\", low_memory = False)\n\n# Fill missing responses with \".\" so that they can be counted and categorized as missing later on\ntrain_no_blanks = train.fillna('.')\ntest_no_blanks = test.fillna('.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Training set:"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "# Treating missing \nfrequency_train_data = pd.DataFrame()\n\nfor i in list(train_no_blanks)[1:]:\n    grouped_data = train_no_blanks.groupby(i)[\"id\"].count()\n    num_unique_answers = grouped_data.size\n    temp_dict = {\"Question\": [i] * num_unique_answers,\n                 \"Response\": list(grouped_data.index),\n                 \"Frequency\": list(grouped_data)}\n    frequency_train_data = frequency_train_data.append(pd.DataFrame(temp_dict))\n\nfrequency_train_data.reset_index(inplace=True, drop=True)\ntemp = frequency_train_data.groupby([\"Question\",\"Response\"]).agg({\"Frequency\":\"sum\"})\ntemp = temp.groupby(level=0).apply(lambda x: 100*x/float(x.sum()))\ntemp.reset_index(inplace=True)\ntemp = temp.rename(columns = {\"Frequency\":\"Relative Frequency (%)\"})\nfrequency_train_data = frequency_train_data.merge(temp, left_on = [\"Question\",\"Response\"], right_on = [\"Question\",\"Response\"], how = \"left\")\n\n# If response contains a \".\" (., .a, .b, etc) then categorize as missing a response\nfrequency_train_data['Response Missing'] = pd.np.where(frequency_train_data.Response.str.find(\".\") > -1, 1, 0)\n\n# Calculate # missing for each question\nmissing_value_df = frequency_train_data.groupby([\"Question\",\"Response\",\"Response Missing\"]).agg({\"Frequency\":\"sum\"})\nmissing_value_df = missing_value_df.groupby(level=0).apply(lambda x: 100*x/float(x.sum()))\nmissing_value_df = missing_value_df.groupby([\"Question\",\"Response Missing\"]).agg({\"Frequency\":\"sum\"})\nmissing_value_df = missing_value_df[missing_value_df.index.get_level_values(\"Response Missing\")==1]\nmissing_value_df = missing_value_df[missing_value_df.index.get_level_values(\"Question\")!=\"satisfied\"]\nmissing_value_df.reset_index(inplace=True)\nmissing_value_df = missing_value_df.rename(columns = {\"Frequency\":\"Percentage Missing\"}).drop(\"Response Missing\", axis=1)\n\n# Rank missing and drop\npercent = 30\ncols_missing = missing_value_df[missing_value_df[\"Percentage Missing\"] > percent]\nn_cols_missing = len(missing_value_df[missing_value_df[\"Percentage Missing\"] > percent])\n\ndrop_missing = cols_missing.Question.to_list()\ntrain_v2 = train_no_blanks.drop(columns = drop_missing)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "# Write data\ntrain_v2.to_csv(\"train_processed_1.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Test set:"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": "# Treating missing \nfrequency_test_data = pd.DataFrame()\n\nfor i in list(test_no_blanks)[1:]:\n    grouped_data = test_no_blanks.groupby(i)[\"id\"].count()\n    num_unique_answers = grouped_data.size\n    temp_dict = {\"Question\": [i] * num_unique_answers,\n                 \"Response\": list(grouped_data.index),\n                 \"Frequency\": list(grouped_data)}\n    frequency_test_data = frequency_test_data.append(pd.DataFrame(temp_dict))\n\nfrequency_test_data.reset_index(inplace=True, drop=True)\ntemp = frequency_test_data.groupby([\"Question\",\"Response\"]).agg({\"Frequency\":\"sum\"})\ntemp = temp.groupby(level=0).apply(lambda x: 100*x/float(x.sum()))\ntemp.reset_index(inplace=True)\ntemp = temp.rename(columns = {\"Frequency\":\"Relative Frequency (%)\"})\nfrequency_test_data = frequency_test_data.merge(temp, left_on = [\"Question\",\"Response\"], right_on = [\"Question\",\"Response\"], how = \"left\")\n\n# If response contains a \".\" (., .a, .b, etc) then categorize as missing a response\nfrequency_test_data['Response Missing'] = pd.np.where(frequency_test_data.Response.str.find(\".\") > -1, 1, 0)\n\n# Calculate # missing for each question\nmissing_value_df = frequency_test_data.groupby([\"Question\",\"Response\",\"Response Missing\"]).agg({\"Frequency\":\"sum\"})\nmissing_value_df = missing_value_df.groupby(level=0).apply(lambda x: 100*x/float(x.sum()))\nmissing_value_df = missing_value_df.groupby([\"Question\",\"Response Missing\"]).agg({\"Frequency\":\"sum\"})\nmissing_value_df = missing_value_df[missing_value_df.index.get_level_values(\"Response Missing\")==1]\nmissing_value_df = missing_value_df[missing_value_df.index.get_level_values(\"Question\")!=\"satisfied\"]\nmissing_value_df.reset_index(inplace=True)\nmissing_value_df = missing_value_df.rename(columns = {\"Frequency\":\"Percentage Missing\"}).drop(\"Response Missing\", axis=1)\n\n# Rank missing and drop\npercent = 30\ncols_missing = missing_value_df[missing_value_df[\"Percentage Missing\"] > percent]\nn_cols_missing = len(missing_value_df[missing_value_df[\"Percentage Missing\"] > percent])\n\ndrop_missing = cols_missing.Question.to_list()\ntest_v2 = test_no_blanks.drop(columns = drop_missing)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": "# Write data\ntest_v2.to_csv(\"test_processed_1.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Dataset 2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Training set:"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": "# Group all missing responses and code them as -1\ntrain_v2 = train_v2.replace([\".\", \".a\", \".b\", \".c\", \".d\"], [-1, -1, -1, -1, -1])\n\n# Dealing with non-numeric\ncat_cols = [\"v17\", \"v20\", \"v25\", \"v78\", \"v154\", \"v155\", \"v161\", \"cntry\"]\n\n# change variable type to str\ntrain_v2[cat_cols] = train_v2[cat_cols].astype(str)\n\n# Copy dataset\ntrain_v3 = train_v2.copy()\n\n# Label encoding the country variables\nlabel_encoder = preprocessing.LabelEncoder()\n\nfor col in cat_cols:\n    train_v3[col] = label_encoder.fit_transform(train_v3[col])\n    \n# Convert all other variables to int64\nnum_cols = train_v3.loc[:, ~train_v3.columns.isin(cat_cols)].columns.tolist()\ntrain_v3[num_cols] = train_v3[num_cols].astype(\"int64\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Checks:"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "display(any(train_v3.dtypes == object))\ndisplay(any(train_v3.dtypes == str))"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": "# Write data\ntrain_v3.to_csv(\"train_processed_2.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Test set:"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": "# Group all missing responses and code them as -1\ntest_v2 = test_v2.replace([\".\", \".a\", \".b\", \".c\", \".d\"], [-1, -1, -1, -1, -1])\n\n# Dealing with non-numeric\ncat_cols = [\"v17\", \"v20\", \"v25\", \"v78\", \"v154\", \"v155\", \"v161\", \"cntry\"]\n\n# change variable type to str\ntest_v2[cat_cols] = test_v2[cat_cols].astype(str)\n\n# Copy dataset\ntest_v3 = test_v2.copy()\n\n# Label encoding the country variables\nlabel_encoder = preprocessing.LabelEncoder()\n\nfor col in cat_cols:\n    test_v3[col] = label_encoder.fit_transform(test_v3[col])\n    \n# Convert all other variables to int64\nnum_cols = test_v3.loc[:, ~test_v3.columns.isin(cat_cols)].columns.tolist()\ntest_v3[num_cols] = test_v3[num_cols].astype(\"int64\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Checks:"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "display(any(test_v3.dtypes == object))\ndisplay(any(test_v3.dtypes == str))"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": "# Write data\ntest_v3.to_csv(\"test_processed_2.csv\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
