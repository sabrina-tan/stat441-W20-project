{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport random\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport os"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": "train = pd.read_csv(\"train_stack_stack.csv\")\ntest = pd.read_csv(\"test_stack_stack.csv\")\n\n# Drop unwanted column\n# train = train.drop(\"Unnamed: 0\", axis = 1)\n# test = test.drop(\"Unnamed: 0\", axis = 1)\n\n# We won't touch test until predicting for submission\n\n# We don't want the ID to be used in the model so preserve\ntrain_id = train['id']\ntest_id = test['id']\n\ntrain = train.drop('id', axis = 1)\ntest = test.drop('id', axis = 1)\n\n# Split X and y\nX = train.loc[:, train.columns != \"satisfied\"]\ny = train.satisfied"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": "# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 3096)"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nLGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n"
    }
   ],
   "source": "# Fit model - default params\nmodel = lgb.LGBMClassifier()\nmodel.fit(X_train, y_train)\nprint(); print(model)"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n              precision    recall  f1-score   support\n\n           0       0.80      0.78      0.79      2824\n           1       0.81      0.83      0.82      3192\n\n    accuracy                           0.81      6016\n   macro avg       0.81      0.80      0.80      6016\nweighted avg       0.81      0.81      0.81      6016\n\n\n[[2207  617]\n [ 550 2642]]\n\n0.8046049081627582\n"
    }
   ],
   "source": "# Predict\nexpected_y = y_test\npredicted_y = model.predict(X_test)\n\n# Summarize model fit\nprint(); print(metrics.classification_report(expected_y, predicted_y))\nprint(); print(metrics.confusion_matrix(expected_y, predicted_y))\nprint(); print(metrics.roc_auc_score(expected_y, predicted_y))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Hyperparameter tuning:"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameter grid\nparam_grid = {\n    'boosting_type': ['gbdt'], \n    'n_estimators': list(range(50,2000)),\n    'num_leaves': list(range(20, 100)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'objective': ['binary'],\n    'metric': ['auc'],\n    'is_unbalance': [False]\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    5.5s\n[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    9.8s\n[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   11.4s\n[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   15.7s\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   19.2s\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.1s\n[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   36.1s\n[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   44.3s\n[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   56.9s\n[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.1min\n[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.2min\n[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.3min\n[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  1.5min\n[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.6min\n[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  1.8min\n[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.9min\n[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  2.3min\n[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  2.6min\n[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  2.7min\n[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  2.9min\n[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  3.0min\n[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  3.2min\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  3.5min\n[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  3.7min\n[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:  4.0min\n"
    }
   ],
   "source": "rsearch = RandomizedSearchCV(estimator = model,\n                             param_distributions = param_grid,\n                             n_iter = 100,\n                             verbose = 10,\n                             cv = 5,\n                             n_jobs = -1,\n                             scoring = 'roc_auc')\n\nrsearch.fit(X_train, y_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(rsearch.best_params_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#Prediction\ny_pred = rsearch.predict(X_test)\n\n#auc calculation\nmetrics.roc_auc_score(y_test,y_pred)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature importance for top 50 predictors\npredictors = [x for x in X_train.columns]\nfeat_imp = pd.Series(rsearch.best_estimator_.feature_importances_, predictors).sort_values(ascending=False)\nfeat_imp = feat_imp[0:50]\nplt.rcParams['figure.figsize'] = 20, 5\nfeat_imp.plot(kind='bar', title='Feature Importance')\nplt.ylabel('Feature Importance Score')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Output to stacking predictions folder:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# train_label = train['satisfied']\n# del train['satisfied']\n\n# Two sets of predictions: one on the training set (which we can use as a feature), one on the test set \n# train_new = train.copy()\ntest_new = test.copy()\n\n# train_preds = pd.DataFrame(rsearch.predict_proba(train_new))\ntest_preds = pd.DataFrame(rsearch.predict_proba(test_new))\n\n# train_new['Predicted_LGBM'] = train_preds[1]\ntest_new['Predicted'] = test_preds[1]\n\n# train_new['id'] = train_id\ntest_new['id'] = test_id\n\n# train_new[['id', 'Predicted_LGBM']].to_csv('/Users/sabrinatan/Desktop/STAT 441/stat441-W20-project/04-modeling/stacking_predictions/train_fe_data_Final_LGBM_no_rounding.csv', index = False, float_format = \"%.8f\")\ntest_new[['id', 'Predicted']].to_csv('/Users/sabrinatan/Desktop/STAT 441/stat441-W20-project/04-modeling/stacking_predictions/LGBM_stack_stack.csv', index = False, float_format = \"%.8f\")\n\n"
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
